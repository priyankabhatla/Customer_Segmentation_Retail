{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6680,"sourceType":"datasetVersion","datasetId":4329}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #0000ff; padding: 10px 20px; border-radius: 20px; box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1); text-align: center;\">\n  <h1 style=\"color: white; font-size: 30px;\">Initial Data Analysis</h1>\n</div>\n","metadata":{}},{"cell_type":"code","source":"### Reading the file\n\ndata=pd.read_excel('/kaggle/input/online-retail/Online Retail.xlsx')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#bbf2ef ; border-radius: 10px; padding: 15px; border: 1px solid #ccc; font-size: 16px;\">\n\n**This DataFrame contains 8 variables that correspond to:**\n\n- **InvoiceNo**  \n  **Type:** Nominal  \n  **Description:** Invoice number. A 6-digit integral number uniquely assigned to each transaction. If this code starts with the letter 'C', it indicates a cancellation.\n\n- **StockCode**  \n  **Type:** Nominal  \n  **Description:** Product (item) code. A 5-digit integral number uniquely assigned to each distinct product.\n\n- **Description**  \n  **Type:** Nominal  \n  **Description:** Product (item) name.\n\n- **Quantity**  \n  **Type:** Numeric  \n  **Description:** The quantities of each product (item) per transaction.\n\n- **InvoiceDate**  \n  **Type:** Numeric  \n  **Description:** Invoice date and time. The day and time when each transaction was generated.\n\n- **UnitPrice**  \n  **Type:** Numeric  \n  **Description:** Unit price. Product price per unit in sterling.\n\n- **CustomerID**  \n  **Type:** Nominal  \n  **Description:** Customer number. A 5-digit integral number uniquely assigned to each customer.\n\n- **Country**  \n  **Type:** Nominal  \n  **Description:** Country name. The name of the country where each customer resides.\n\n</div>\n","metadata":{}},{"cell_type":"code","source":"### Understanding the structure of dataset\n\ndata.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Percentage of null records in description column\n\nprint(\"Percentage of null records in description column\",round((1454/541909)*100,2),\"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Percentage of null records in Customer ID column\n\nprint(\"Percentage of null records in CustomerID column\",round((135080/541909)*100,2),\"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #bbf2ef; border-radius: 10px; padding: 15px; border: 1px solid #ccc; font-size: 16px;\">\n\n### Initial Inferences From Data:\n\n* Dataset contains 541,909 entries and 8 columns\n* **Invoice No:** 1 invoice number can have multiple products purchased\n* **Description:** ~0.3% of descriptions present are nulls \n* **Customer ID:** ~25% of Customer IDs are nulls\n\n</div>\n","metadata":{}},{"cell_type":"code","source":"### Descriptive Statistics\n\ndata.describe().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Summary statistics for categorical variables\n\ndata.describe(include='object').T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #bbf2ef ; border-radius: 10px; padding: 15px; border: 1px solid #ccc; font-size: 16px;\">\n\n### Inferences\n\n#### Quantity\n* **Average Product Quantity per transaction:** 9.55\n* **Negative Values:** Indicates cancelled orders.\n* **High Standard Deviation (218.08):** Data is widely spread out.\n* **Outliers:** Presence of outliers due to a significant gap between the 75th percentile and maximum value.\n\n#### UnitPrice\n* **Average Unit Price:** 4.61\n* **Outliers:** Presence of outliers due to a significant gap between the 75th percentile and maximum value.\n\n#### CustomerID\n* **Missing Values:** 406,829 missing values.\n\n#### StockCode\n* **Unique Stock Codes:** 4,070 unique stock codes.\n\n#### Description\n* **Unique Descriptions:** 4,223 unique descriptions.\n* **Most Frequent Description:** \"White hanging heart t-light holder\" (2,369 times).\n* **Missing Values:** Few missing values.\n\n#### Country\n* **Data Coverage:** Data from 38 different countries is available.\n\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color: #0000ff; padding: 10px 20px; border-radius: 20px; box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1); text-align: center;\">\n  <h1 style=\"color: white; font-size: 30px;\">Data Cleaning and Transformation</h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color: #3296e3; padding: 8px 16px; border-radius: 20px; box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1); text-align: left;\">\n  <h1 style=\"color: white; font-size: 24px;\">Checking for missing values and removing them</h1>\n</div>\n","metadata":{}},{"cell_type":"code","source":"### Checking for nulls\n\nmissing_data=data.isna().sum()\nmissing_per=(missing_data[missing_data>0]/data.shape[0])*100\nmissing_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Percentage of missing values\n\nround(missing_per,2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Checking if for each stock code - only 1 description exists or not\n\ndata_stockcodes=data.groupby('StockCode')['Description'].unique()\ndata_stockcodes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #bbf2ef; border-radius: 10px; padding: 15px; border: 1px solid #ccc; font-size: 16px;\">\n\n### Inferences\n\n#### Customer ID\n* ~ 25% of the customer IDs are missing.\n* Imputing data won't help as clustering is based on customer behavior and preferences. Accurate data is required.\n* Removing them might be a good choice.\n\n#### Description\n* 0.27% of descriptions are missing.\n* Each stock code does not correspond to a unique description.\n* Removing this column might be a good choice as well.\n\n</div>\n","metadata":{}},{"cell_type":"code","source":"### Removing rows with missing values in 'CustomerID' and 'Description' columns\n\ndata = data.dropna(subset=['CustomerID', 'Description'])\ndata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #3296e3; padding: 8px 16px; border-radius: 20px; box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1); text-align: left;\">\n  <h1 style=\"color: white; font-size: 24px;\">Handling Duplicate Values</h1>\n</div>\n","metadata":{}},{"cell_type":"code","source":"### Checking for duplicate entries\n\ndata.duplicated().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Deleting the duplicate entries\n\ndata.drop_duplicates(inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #3296e3; padding: 8px 16px; border-radius: 20px; box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1); text-align: left;\">\n  <h1 style=\"color: white; font-size: 24px;\"> 'Invoice number' having 'C' as prefix signifies cancelled transactions </h1>\n</div>\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\n\n### Creating a regular expression pattern\npattern = r'^[a-zA-Z]+\\d+$|\\d+[a-zA-Z]+$'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Filtering rows based on the pattern\ncancelled_df = data[data['InvoiceNo'].str.match(pattern) == True] \ncancelled_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cancelled_df[['Quantity', 'UnitPrice']].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #bbf2ef ; border-radius: 10px; padding: 15px; border: 1px solid #ccc; font-size: 16px;\">\n\n### Inferences\n\n* Unit prices have a high variance factor. \n* Keeping these records might help in clustering as this would efficiently capture the segment of customers that cancel their orders often\n* Recommendation system will not recommend these products\n\n</div>\n","metadata":{}},{"cell_type":"code","source":"### Creating a new column to show transaction status\n\ndata['Transaction_Status'] = np.where(data['InvoiceNo'].astype(str).str.startswith('C'), 'Cancelled', 'Completed')\ndata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[data['Transaction_Status']==\"Cancelled\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Percentage of Cancelled Transactions\n\ncancelled_percent=round(((data['Transaction_Status'] == 'Cancelled').sum()/data.shape[0])*100,2)\nprint(\"The percentage of cancelled transactions is: \",cancelled_percent,\"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #3296e3; padding: 8px 16px; border-radius: 20px; box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1); text-align: left;\">\n  <h1 style=\"color: white; font-size: 24px;\"> Analysing the Stockcode Column  </h1>\n</div>\n","metadata":{}},{"cell_type":"code","source":"### Unique Stock Codes\n\nstock_codes_distinct=data['StockCode'].nunique()\nstock_codes_distinct","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Value counts for each Stock Code\n\ndata['StockCode'].value_counts().head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Plotting the top 10 stock codes sold\n\nimport matplotlib.pyplot as plt\n\ntop_stock_codes = data['StockCode'].value_counts().head(10)\n\ntop_stock_codes.plot(kind='bar', figsize=(10, 6))\nplt.title('Top 10 Stock Codes')\nplt.xlabel('Stock Code')\nplt.ylabel('Frequency')\nplt.xticks(rotation=0, fontsize=10)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #bbf2ef ; border-radius: 10px; padding: 15px; border: 1px solid #ccc; font-size: 16px;\">\n\n### Inferences\n\n* Not all stock codes are numeric \n* Need to dive further to understand how many non numeric stock codes exist\n\n</div>","metadata":{}},{"cell_type":"code","source":"data.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Finding the number of numeric characters in each unique stock code\n\nunique_stock_codes = data['StockCode'].unique()\nnumeric_char_counts_in_unique_codes = pd.Series(unique_stock_codes).apply(lambda x: sum(c.isdigit() for c in str(x))).value_counts()\nnumeric_char_counts_in_unique_codes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_stock_codes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Finding and printing the stock codes with 0 and 1 numeric characters\n\nanomalous_stock_codes = [code for code in unique_stock_codes if sum(c.isdigit() for c in str(code)) in (0, 1)]\nprint(\"Anomalous stock codes:\")\nfor code in anomalous_stock_codes:\n    print(code)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Pecentage of anomalous records present in data\n\nanomalous_stock_codes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Filter the DataFrame for anomalous stock codes\n\nanomalous_data = data[data['StockCode'].isin(anomalous_stock_codes)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Calculate the sum of records\n\ntotal_anomalous_records = len(anomalous_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Getting percentage of anomalous codes in the data\n\nper_anomalous_codes=round((total_anomalous_records/data.shape[0])*100,2)\nprint(\"Percentage of anomalous codes in the dataframe:\", per_anomalous_codes,\"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #bbf2ef ; border-radius: 10px; padding: 15px; border: 1px solid #ccc; font-size: 16px;\">\n\n### Inferences\n\n* Majority of the codes have 5 digits\n* 7 anomalous codes only exist in 0.48% of the dataset and hence can be removed\n\n\n</div>","metadata":{}},{"cell_type":"code","source":"anomalous_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Filtering the DataFrame for normal stock codes\n\ndata = data[~data['StockCode'].isin(anomalous_stock_codes)]\ndata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #3296e3; padding: 8px 16px; border-radius: 20px; box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1); text-align: left;\">\n  <h1 style=\"color: white; font-size: 24px;\"> Analysing the Description Column </h1>\n</div>\n","metadata":{}},{"cell_type":"code","source":"### Top 20 selling products\n\ntop_product_desc = data['Description'].value_counts().head(20).sort_values(ascending=False)\n\ntop_product_desc.plot(kind='barh', figsize=(10, 6))\nplt.title('Top 20 Product Descriptions')\nplt.xlabel('Frequency')\nplt.ylabel('Product Description')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Finding unique descriptions containing lowercase characters\n\nlowercase_descriptions = data['Description'].unique()\nlowercase_descriptions = [desc for desc in lowercase_descriptions if any(char.islower() for char in desc)]\nfor desc in lowercase_descriptions:\n    print(desc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Checking for product descriptions containing the following keywords: \"Next Day Carriage\" OR \"High Resolution Image\"\n\nfiltered_data = data[(data['Description'] == 'Next Day Carriage') | (data['Description'] == 'High Resolution Image')]\nfiltered_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Removing the above records as these terms don't make sense for product descriptions\n\npercent_anom_pd=round((filtered_data.shape[0]/data.shape[0])*100,2)\nprint(\"Percentage of instances with 'Next Day Carriage' or 'High resolution Image' are\",percent_anom_pd,\"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filtered_data['Description'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Removing the above records as these terms don't make sense for product descriptions\n\ndata = data[~data['Description'].isin(filtered_data['Description'].unique())]\ndata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #3296e3; padding: 8px 16px; border-radius: 20px; box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1); text-align: left;\">\n  <h1 style=\"color: white; font-size: 24px;\"> Analysing the Unit Price Column </h1>\n</div>","metadata":{}},{"cell_type":"code","source":"data['UnitPrice'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[data['UnitPrice']==0].describe()[['Quantity']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #bbf2ef ; border-radius: 10px; padding: 15px; border: 1px solid #ccc; font-size: 16px;\">\n\n### Inferences\n\n* Unit Price=0 in 33 records (Might be an error)\n* Deep dive: Qty is 12540 where Unit Price is 0. Doesn't seem right\n* Removing these record should be a better approach to remove noise\n\n</div>","metadata":{}},{"cell_type":"code","source":"### Removing these records from the data\n\ndata=data[data['UnitPrice']!=0]\ndata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Resetting Index\n\ndata.reset_index(drop=True,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #0000ff; padding: 10px 20px; border-radius: 20px; box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1); text-align: center;\">\n  <h1 style=\"color: white; font-size: 30px;\"> Feature Engineering</h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color: #3296e3; padding: 8px 16px; border-radius: 20px; box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1); text-align: left;\">\n  <h1 style=\"color: white; font-size: 24px;\"> Recency (Days Since Last Purchase) </h1>\n</div>","metadata":{}},{"cell_type":"code","source":"data.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Extracting date\n\ndata['InvoiceDay'] = data['InvoiceDate'].dt.date","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Creating a new dataframe for features at customer ID level\n\ncustomer_data=data.groupby('CustomerID')['InvoiceDay'].max().reset_index()\ncustomer_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Extracting the most recent date of purchase and subtracting it \n\nmost_recent_dateofpurchase=data['InvoiceDay'].max()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Converting them to same date format\n\ncustomer_data['InvoiceDay'] = pd.to_datetime(customer_data['InvoiceDay'])\nmost_recent_dateofpurchase = pd.to_datetime(most_recent_dateofpurchase)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Calculating the Recency of each customer\n\ncustomer_data['Days_Since_Last_Purchase'] = (most_recent_dateofpurchase - customer_data['InvoiceDay']).dt.days\ncustomer_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Removing Invoice Day\n\ncustomer_data.drop(columns=['InvoiceDay'], inplace=True)\ncustomer_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #3296e3; padding: 8px 16px; border-radius: 20px; box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1); text-align: left;\">\n  <h1 style=\"color: white; font-size: 24px;\"> Frequency (Total Transaction And Total Products Purchased) </h1>\n</div>","metadata":{}},{"cell_type":"code","source":"data['CustomerID'].nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Total Transaction per customer\n\ntransactions_df=data.groupby('CustomerID')['InvoiceNo'].nunique().reset_index()\ntransactions_df.rename(columns={'InvoiceNo': 'Total_Transactions'}, inplace=True)\ntransactions_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Total Products purchased per customer\n\nproducts_purchased_df=data.groupby('CustomerID')['Quantity'].sum().reset_index()\nproducts_purchased_df.rename(columns={'Quantity': 'Total_Products_Purchased'}, inplace=True)\nproducts_purchased_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Merge the new features into the customer_data dataframe\n\ncustomer_data = pd.merge(customer_data, transactions_df, on='CustomerID')\ncustomer_data = pd.merge(customer_data, products_purchased_df, on='CustomerID')\ncustomer_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #3296e3; padding: 8px 16px; border-radius: 20px; box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1); text-align: left;\">\n  <h1 style=\"color: white; font-size: 24px;\"> Monetary (Total Spend, Average Order Value) </h1>\n</div>","metadata":{}},{"cell_type":"code","source":"### Total Spend per customer\n\ndata['TotalSpend']=data['UnitPrice']*data['Quantity']\ntotal_spend_df=data.groupby('CustomerID')['TotalSpend'].sum().reset_index()\ntotal_spend_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Merging the new features\n\ncustomer_data = pd.merge(customer_data, total_spend_df, on='CustomerID')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customer_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### AOV per customer\n\ncustomer_data['AOV']=customer_data['TotalSpend']/customer_data['Total_Transactions']\ncustomer_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #3296e3; padding: 8px 16px; border-radius: 20px; box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1); text-align: left;\">\n  <h1 style=\"color: white; font-size: 24px;\"> Product Diversity (Unique Products Purchased) </h1>\n</div>","metadata":{}},{"cell_type":"code","source":"### Calculating the number of unique products purchased by each customer\n\nunique_products_purchased = data.groupby('CustomerID')['StockCode'].nunique().reset_index()\nunique_products_purchased.rename(columns={'StockCode': 'Unique_Products_Purchased'}, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Merging the new feature into the customer_data dataframe\n\ncustomer_data = pd.merge(customer_data, unique_products_purchased, on='CustomerID')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Displaying the first few rows of the customer_data dataframe\n\ncustomer_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #3296e3; padding: 8px 16px; border-radius: 20px; box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1); text-align: left;\">\n  <h1 style=\"color: white; font-size: 24px;\"> Behavioral Features(Average Days Between Purchase, Favourite Shopping Day, Favourite Shopping Hour) </h1>\n</div>","metadata":{}},{"cell_type":"code","source":"data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Extracting day of week and hour from InvoiceDate\n\ndata['Day_Of_Week'] = data['InvoiceDate'].dt.dayofweek\ndata['Hour'] = data['InvoiceDate'].dt.hour","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Calculating the average number of days between consecutive purchases \n\ndays_between_purchases = data.groupby('CustomerID')['InvoiceDay'].apply(lambda x: (x.diff().dropna()).apply(lambda y: y.days))\naverage_days_between_purchases = days_between_purchases.groupby('CustomerID').mean().reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Renaming the column before merging\n\naverage_days_between_purchases.rename(columns={'InvoiceDay': 'Average_Days_Between_Purchases'}, inplace=True)\ncustomer_data = pd.merge(customer_data, average_days_between_purchases, on='CustomerID')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Finding the favorite shopping day of the week\n\nfavorite_shopping_day = data.groupby(['CustomerID', 'Day_Of_Week']).size().reset_index(name='Count')\nfavorite_shopping_day = favorite_shopping_day.loc[favorite_shopping_day.groupby('CustomerID')['Count'].idxmax()][['CustomerID', 'Day_Of_Week']]\ncustomer_data = pd.merge(customer_data, favorite_shopping_day, on='CustomerID')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Finding the favorite shopping hour of the day\n\nfavorite_shopping_hour = data.groupby(['CustomerID', 'Hour']).size().reset_index(name='Count')\nfavorite_shopping_hour = favorite_shopping_hour.loc[favorite_shopping_hour.groupby('CustomerID')['Count'].idxmax()][['CustomerID', 'Hour']]\ncustomer_data = pd.merge(customer_data, favorite_shopping_hour, on='CustomerID')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customer_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #3296e3; padding: 8px 16px; border-radius: 20px; box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1); text-align: left;\">\n  <h1 style=\"color: white; font-size: 24px;\"> Geographic Features(Country) </h1>\n</div>","metadata":{}},{"cell_type":"code","source":"data['Country'].value_counts(normalize=True).head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #bbf2ef ; border-radius: 10px; padding: 15px; border: 1px solid #ccc; font-size: 16px;\">\n\n### Inferences\n\n* UK seems to have the highest number of records\n* Creating a dominant country for each customer would be a good idea to indicate if a customer makes maximum number of transactions in UK or not\n\n</div>","metadata":{}},{"cell_type":"code","source":"### Number of transactions per country for each customer\n\ncustomer_country = data.groupby(['CustomerID', 'Country']).size().reset_index(name='Number_of_Transactions')\ncustomer_country","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Checking for duplicate entries\n\ncustomer_country[customer_country['CustomerID'].duplicated(keep=False)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Country with the maximum number of transactions for each customer \n\ncustomer_main_country = customer_country.sort_values('Number_of_Transactions', ascending=False).drop_duplicates('CustomerID')\ncustomer_main_country","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customer_main_country[customer_main_country['CustomerID']==12370]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Creating a binary column indicating whether the customer is from the UK or not\n\ncustomer_main_country['Is_UK'] = customer_main_country['Country'].apply(lambda x: 1 if x == 'United Kingdom' else 0)\ncustomer_main_country","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Merging this data with our customer_data dataframe\n\ncustomer_data = pd.merge(customer_data, customer_main_country[['CustomerID', 'Is_UK']], on='CustomerID', how='left')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customer_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customer_data['Is_UK'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #3296e3; padding: 8px 16px; border-radius: 20px; box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1); text-align: left;\">\n  <h1 style=\"color: white; font-size: 24px;\"> Cancellations( Cancellation Frequency, Cancellation Rate)  </h1>\n</div>","metadata":{}},{"cell_type":"code","source":"customer_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Getting the cancelled transactions per customer\n\nCancelled_transactions=data[data['Transaction_Status']==\"Cancelled\"]\nCancelled_transactions.head(4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Creating a column for the cancellation frequency\n\ncancelled_frequency=Cancelled_transactions.groupby('CustomerID')['InvoiceNo'].nunique().reset_index()\ncancelled_frequency","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Renaming the column \n\ncancelled_frequency.rename(columns={'InvoiceNo': 'Cancellation_Frequency'}, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Merging the Cancellation Frequency data into the customer_data dataframe\n\ncustomer_data = pd.merge(customer_data, cancelled_frequency, on='CustomerID', how='left')\ncustomer_data.head(4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Replacing NaN values with 0\n\ncustomer_data['Cancellation_Frequency'].fillna(0, inplace=True)\ncustomer_data.head(4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Calculating the Cancellation Rate\n\ncustomer_data['Cancellation_Rate'] = customer_data['Cancellation_Frequency'] / customer_data['Total_Transactions']\ncustomer_data.head(4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #3296e3; padding: 8px 16px; border-radius: 20px; box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1); text-align: left;\">\n  <h1 style=\"color: white; font-size: 24px;\"> Seasonality (Monthly Spending Mean, Monthly Spending SD, Spending Trend) </h1>\n</div>","metadata":{}},{"cell_type":"code","source":"### Extracting month and year from InvoiceDate\n\ndata['Year'] = data['InvoiceDate'].dt.year\ndata['Month'] = data['InvoiceDate'].dt.month","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"monthly_spend=data.groupby(['CustomerID','Year','Month'])['TotalSpend'].sum().reset_index()\nmonthly_spend","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Calculating mean \n\nmonthly_spend_mean=monthly_spend.groupby(['CustomerID'])['TotalSpend'].agg(['mean']).reset_index()\nmonthly_spend_mean.rename(columns={'mean':'Monthly_Spending_Mean'},inplace=True)\nmonthly_spend_mean","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Calculating stdev \n\nmonthly_spend_sd=monthly_spend.groupby(['CustomerID'])['TotalSpend'].agg(['std']).reset_index()\nmonthly_spend_sd.rename(columns={'std':'Monthly_Spending_SD'},inplace=True)\nmonthly_spend_sd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Replace NaN values with 0\n\nmonthly_spend_sd['Monthly_Spending_SD'].fillna(0, inplace=True)\nmonthly_spend_sd.head(4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Merging the new features into the customer_data dataframe\n\ncustomer_data = pd.merge(customer_data, monthly_spend_mean, on='CustomerID')\ncustomer_data = pd.merge(customer_data, monthly_spend_sd, on='CustomerID')\ncustomer_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Calculating Spending Trends using Linear Regression\n\nfrom scipy.stats import linregress\n\ndef calculate_trend(spend_data):\n    if len(spend_data) > 1:\n        x = np.arange(len(spend_data))\n        slope, _, _, _, _ = linregress(x, spend_data)\n        return slope\n    else:\n        return 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Apply the calculate_trend function to find the spending trend for each customer\n\nspending_trends = monthly_spend.groupby('CustomerID')['TotalSpend'].apply(calculate_trend).reset_index()\nspending_trends.rename(columns={'TotalSpend': 'Spending_Trend'}, inplace=True)\nspending_trends","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #bbf2ef ; border-radius: 10px; padding: 15px; border: 1px solid #ccc; font-size: 16px;\">\n\n### Inferences\n\n* Positive values in spending trend indicates possibility to growing loyalty/satisfied customer. The opposite would hint at customer attrition\n\n</div>","metadata":{}},{"cell_type":"code","source":"### Merge the new features into the customer_data dataframe\n\ncustomer_data = pd.merge(customer_data, spending_trends, on='CustomerID')\ncustomer_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Changing the data type of 'CustomerID' to string as it is a unique identifier and not used in mathematical operations\ncustomer_data['CustomerID'] = customer_data['CustomerID'].astype(str)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Convert data types of columns to optimal types\n\ncustomer_data = customer_data.convert_dtypes()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customer_data.head(4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customer_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Customer Dataset Description (Updated):\n\n<div style=\"background-color:#bbf2ef ; border-radius: 10px; padding: 15px; border: 1px solid #ccc; font-size: 16px;\">\n\n**This DataFrame contains 15 variables that correspond to:**\n\n- **CustomerID:** Identifier uniquely assigned to each customer, used to distinguish individual customers.\n\n- **Days_Since_Last_Purchase:**The number of days that have passed since the customer's last purchase.\n\n- **Total_Transactions:** The total number of transactions made by the customer.\n\n- **Total_Products_Purchased`:** The total quantity of products purchased by the customer across all transactions.\n\n- **Total_Spend:** The total amount of money the customer has spent across all transactions.\n\n- **AOV:** Average Order Value, calculated as the total spend divided by the number of transactions.\n\n- **Unique_Products_Purchased:** The number of different products the customer has purchased.\n\n- **Average_Days_Between_Purchases:** The average number of days between consecutive purchases made by the customer.\n\n- **Day_Of_Week:** The day of the week when the customer prefers to shop, represented numerically (0 for Monday, 6 for Sunday).\n\n- **Hour:** The hour of the day when the customer prefers to shop, represented in a 24-hour format.\n\n- **Is_UK:** A binary variable indicating whether the customer is based in the UK (1) or not (0).\n\n- **Cancellation_Frequency:** The total number of transactions that the customer has cancelled.\n\n- **Cancellation_Rate:** The proportion of transactions that the customer has cancelled, calculated as cancellation frequency divided by total transactions.\n\n- **Monthly_Spending_Mean:** The average monthly spending of the customer.\n\n- **Monthly_Spending_SD:** The standard deviation of the customer's monthly spending, indicating the variability in their spending pattern.\n\n- **Spending_Trend:** A numerical representation of the trend in the customer's spending over time. A positive value indicates an increasing trend, a negative value indicates a decreasing trend, and a value close to zero indicates a stable trend.\n\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color: #0000ff; padding: 10px 20px; border-radius: 20px; box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1); text-align: center;\">\n  <h1 style=\"color: white; font-size: 30px;\"> Outlier Detection And Treatment</h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color: #bbf2ef ; border-radius: 10px; padding: 15px; border: 1px solid #ccc; font-size: 16px;\">\n\nSince K-Means is very sensitive to outliers - we'll use **Isolation forest Algorithm** to deal with outliers. It isolates observations by randomly selecting a feature and then randomly selects a split value between the maximum and minimum values of selected feature\n\n**Strategy**: Flag Outliers in another column and exclude those from our dataset\n\n</div>","metadata":{}},{"cell_type":"code","source":"### Initializing the IsolationForest model \n\nfrom sklearn.ensemble import IsolationForest\n\nmodel = IsolationForest(contamination=0.05, random_state=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Fitting the model on our database\n\ncustomer_data['Outlier_Scores'] = model.fit_predict(customer_data.iloc[:, 1:].to_numpy())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Creating a new column to identify outliers (1 for inliers and -1 for outliers)\n\ncustomer_data['Is_Outlier'] = [1 if x == -1 else 0 for x in customer_data['Outlier_Scores']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Displaying the first few rows of the customer_data dataframe\n\ncustomer_data.head(4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Calculate the percentage of inliers and outliers\n\noutlier_percentage = customer_data['Is_Outlier'].value_counts(normalize=True) * 100\noutlier_percentage","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #bbf2ef ; border-radius: 10px; padding: 15px; border: 1px solid #ccc; font-size: 16px;\">\n\n### Inferences\n\n* 5% outliers present in the data\n\n</div>","metadata":{}},{"cell_type":"code","source":"### Separating the outliers for analysis\n\noutliers_data = customer_data[customer_data['Is_Outlier'] == 1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Removing the outliers from the main dataset\n\ncustomer_data_cleaned = customer_data[customer_data['Is_Outlier'] == 0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Dropping the 'Outlier_Scores' and 'Is_Outlier' columns\n\ncustomer_data_cleaned = customer_data_cleaned.drop(columns=['Outlier_Scores', 'Is_Outlier'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Resetting the index of the cleaned data\n\ncustomer_data_cleaned.reset_index(drop=True, inplace=True)\ncustomer_data_cleaned.head(4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customer_data_cleaned.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #0000ff; padding: 10px 20px; border-radius: 20px; box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1); text-align: center;\">\n  <h1 style=\"color: white; font-size: 30px;\"> Correlation Analysis </h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color: #bbf2ef ; border-radius: 10px; padding: 15px; border: 1px solid #ccc; font-size: 16px;\">\n\nWe need to check for presence of **multicollinearity** before proceeding to the clustering\n\n</div>","metadata":{}},{"cell_type":"code","source":"### Calculating the correlation matrix excluding the 'CustomerID' column\n\ncorr = customer_data_cleaned.drop(columns=['CustomerID']).corr()\ncorr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nfrom matplotlib.colors import LinearSegmentedColormap\nfrom matplotlib import colors as mcolors\n\n### Reset background style\nsns.set_style('whitegrid')\n\n### Defining a custom colormap\ncolors = ['#2662ed', '#26dced', '#087cbf','#dce3de','white']\nmy_cmap = LinearSegmentedColormap.from_list('custom_map', colors, N=256)\n\n### Creating a mask to only show the lower triangle of the matrix \nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask, k=1)] = True\n\n### Plotting the heatmap\nplt.figure(figsize=(12, 10))\nsns.heatmap(corr, mask=mask, cmap=my_cmap, annot=True, center=0, fmt='.2f', linewidths=2)\nplt.title('Correlation Matrix', fontsize=14)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #bbf2ef ; border-radius: 10px; padding: 15px; border: 1px solid #ccc; font-size: 16px;\">\n\n### Inferences\n\n* Highly correlated variables are as follows:\n    * **Monthly Spending Mean and AOV** (0.80)\n    * **Total_Spend and Total_Products_Purchased** (0.88)\n    * **Total_Transactions and Total_Spend** (0.80)\n    * **Total_Transactions and Total_Products_Purchased** (0.71)\n    * **Cancellation_Rate and Cancellation_Frequency** (0.69)\n\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color: #0000ff; padding: 10px 20px; border-radius: 20px; box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1); text-align: center;\">\n  <h1 style=\"color: white; font-size: 30px;\"> Feature Scaling </h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color: #bbf2ef ; border-radius: 10px; padding: 15px; border: 1px solid #ccc; font-size: 16px;\">\n\n* Before proceeding to PCA, it is important to scale all the features\n* **Features to be ignored:** CustomerID, Is_UK, Day_Of_Week\n\n</div>","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n### Initialize the StandardScaler\n\nscaler = StandardScaler()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### List of columns that don't need to be scaled\n\ncolumns_to_exclude = ['CustomerID', 'Is_UK', 'Day_Of_Week']\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### List of columns that need to be scaled\n\ncolumns_to_scale = customer_data_cleaned.columns.difference(columns_to_exclude)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Copying the cleaned dataset\n\ncustomer_data_scaled = customer_data_cleaned.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Applying the scaler to the necessary columns in the dataset\n\ncustomer_data_scaled[columns_to_scale] = scaler.fit_transform(customer_data_scaled[columns_to_scale])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Displaying the first few rows of the scaled data\n\ncustomer_data_scaled.head(4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #0000ff; padding: 10px 20px; border-radius: 20px; box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1); text-align: center;\">\n  <h1 style=\"color: white; font-size: 30px;\"> Dimensionality Reduction (PCA) </h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color: #bbf2ef ; border-radius: 10px; padding: 15px; border: 1px solid #ccc; font-size: 16px;\">\n\n* To deal with multucollinear features, we will go ahead with Principal Component Analysis\n* This would help in forming better clusters\n\n</div>","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\n### Setting CustomerID as the index column\ncustomer_data_scaled.set_index('CustomerID', inplace=True)\n\n### Applying PCA\npca = PCA().fit(customer_data_scaled)\n\n### Calculating the Cumulative Sum of the Explained Variance\nexplained_variance_ratio = pca.explained_variance_ratio_\ncumulative_explained_variance = np.cumsum(explained_variance_ratio)\n\n### Setting the optimal k value (based on our analysis, we can choose 6)\noptimal_k = 6","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Setting seaborn plot style\nsns.set(rc={'axes.facecolor': '#dce3de'}, style='darkgrid')\n\n### Plot the cumulative explained variance against the number of components\nplt.figure(figsize=(20, 10))\n\n### Bar chart for the explained variance of each component\nbarplot = sns.barplot(x=list(range(1, len(cumulative_explained_variance) + 1)),\n                      y=explained_variance_ratio,\n                      color='#2662ed',\n                      alpha=0.8)\n\n### Line plot for the cumulative explained variance\nlineplot, = plt.plot(range(0, len(cumulative_explained_variance)), cumulative_explained_variance,\n                     marker='o', linestyle='--', color='#087cbf', linewidth=2)\n\n### Plot optimal k value line\noptimal_k_line = plt.axvline(optimal_k - 1, color='#26dced', linestyle='--', label=f'Optimal k value = {optimal_k}') \n\n# Set labels and title\nplt.xlabel('Number of Components', fontsize=14)\nplt.ylabel('Explained Variance', fontsize=14)\nplt.title('Cumulative Variance vs. Number of Components', fontsize=18)\n\n### Customize ticks and legend\nplt.xticks(range(0, len(cumulative_explained_variance)))\nplt.legend(handles=[barplot.patches[0], lineplot, optimal_k_line],\n           labels=['Explained Variance of Each Component', 'Cumulative Explained Variance', f'Optimal k value = {optimal_k}'],\n           loc=(0.62, 0.1),\n           frameon=True,\n           framealpha=1.0,  \n           edgecolor='#000203')  \n\n### Display the variance values for both graphs on the plots\nx_offset = -0.3\ny_offset = 0.01\nfor i, (ev_ratio, cum_ev_ratio) in enumerate(zip(explained_variance_ratio, cumulative_explained_variance)):\n    plt.text(i, ev_ratio, f\"{ev_ratio:.2f}\", ha=\"center\", va=\"bottom\", fontsize=10)\n    if i > 0:\n        plt.text(i + x_offset, cum_ev_ratio + y_offset, f\"{cum_ev_ratio:.2f}\", ha=\"center\", va=\"bottom\", fontsize=10)\n\nplt.grid(axis='both')   \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #bbf2ef ; border-radius: 10px; padding: 15px; border: 1px solid #ccc; font-size: 16px;\">\n\n* To deal with multucollinear features, we will go ahead with Principal Component Analysis\n* This would help in forming better clusters\n\n</div>","metadata":{}},{"cell_type":"code","source":"### Creating a PCA object with 6 components\n\npca = PCA(n_components=6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Fitting and transforming the original data to the new PCA dataframe\n\ncustomer_data_pca = pca.fit_transform(customer_data_scaled)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Creating a new dataframe from the PCA dataframe, with columns labeled PC1, PC2, etc.\n\ncustomer_data_pca = pd.DataFrame(customer_data_pca, columns=['PC'+str(i+1) for i in range(pca.n_components_)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Adding the CustomerID index back to the new PCA dataframe\n\ncustomer_data_pca.index = customer_data_scaled.index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customer_data_pca","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Defining a function to highlight the top 3 absolute values in each column of a dataframe\n\ndef highlight_top3(column):\n    top3 = column.abs().nlargest(3).index\n    return ['background-color:  #dce3de' if i in top3 else '' for i in column.index]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Creating the PCA component DataFrame and applying the highlighting function\n\npc_df = pd.DataFrame(pca.components_.T, columns=['PC{}'.format(i+1) for i in range(pca.n_components_)],  \n                     index=customer_data_scaled.columns)\n\npc_df.style.apply(highlight_top3, axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #0000ff; padding: 10px 20px; border-radius: 20px; box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1); text-align: center;\">\n  <h1 style=\"color: white; font-size: 30px;\"> K-Means Clustering </h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color: #3296e3; padding: 8px 16px; border-radius: 20px; box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1); text-align: left;\">\n  <h1 style=\"color: white; font-size: 24px;\"> Optimal Number Of Clusters (Elbow Method) </h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color: #bbf2ef ; border-radius: 10px; padding: 15px; border: 1px solid #ccc; font-size: 16px;\">\n\n* To deal with multucollinear features, we will go ahead with Principal Component Analysis\n* This would help in forming better clusters\n\n</div>","metadata":{}},{"cell_type":"code","source":"from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\nfrom sklearn.cluster import KMeans\n\n### Set plot style, and background color\nsns.set(style='darkgrid', rc={'axes.facecolor': '#dce3de'})\n\n### Set the color palette for the plot\nsns.set_palette(['#2662ed'])\n\n### Instantiate the clustering model with the specified parameters\nkm = KMeans(init='k-means++', n_init=10, max_iter=100, random_state=0)\n\n### Create a figure and axis with the desired size\nfig, ax = plt.subplots(figsize=(12, 5))\n\n### Instantiate the KElbowVisualizer with the model and range of k values, and disable the timing plot\nvisualizer = KElbowVisualizer(km, k=(2, 15), timings=False, ax=ax)\n\n### Fit the data to the visualizer\nvisualizer.fit(customer_data_pca)\n\n### Finalize and render the figure\nvisualizer.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #bbf2ef ; border-radius: 10px; padding: 15px; border: 1px solid #ccc; font-size: 16px;\">\n\n## Inference\n    \n* The k value is coming out to be 6 in this case - however it is a little unclear given there is no clear elbow formed\n\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color: #3296e3; padding: 8px 16px; border-radius: 20px; box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1); text-align: left;\">\n  <h1 style=\"color: white; font-size: 24px;\"> Optimal Number Of Clusters (Silhouette Method) </h1>\n</div>","metadata":{}},{"cell_type":"code","source":"import matplotlib.gridspec as gridspec\nfrom sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n\ndef silhouette_analysis(df, start_k, stop_k, figsize=(15, 16)):\n\n    ### Set the size of the figure\n    plt.figure(figsize=figsize)\n\n    ### Create a grid with (stop_k - start_k + 1) rows and 2 columns\n    grid = gridspec.GridSpec(stop_k - start_k + 1, 2)\n\n    ### Assign the first plot to the first row and both columns\n    first_plot = plt.subplot(grid[0, :])\n\n    ### First plot: Silhouette scores for different k values\n    sns.set_palette(['darkorange'])\n\n    silhouette_scores = []\n\n    ### Iterate through the range of k values\n    for k in range(start_k, stop_k + 1):\n        km = KMeans(n_clusters=k, init='k-means++', n_init=10, max_iter=100, random_state=0)\n        km.fit(df)\n        labels = km.predict(df)\n        score = silhouette_score(df, labels)\n        silhouette_scores.append(score)\n\n    best_k = start_k + silhouette_scores.index(max(silhouette_scores))\n\n    plt.plot(range(start_k, stop_k + 1), silhouette_scores, marker='o')\n    plt.xticks(range(start_k, stop_k + 1))\n    plt.xlabel('Number of clusters (k)')\n    plt.ylabel('Silhouette score')\n    plt.title('Average Silhouette Score for Different k Values', fontsize=15)\n\n    ### Add the optimal k value text to the plot\n    optimal_k_text = f'The k value with the highest Silhouette score is: {best_k}'\n    plt.text(10, 0.23, optimal_k_text, fontsize=12, verticalalignment='bottom', \n             horizontalalignment='left', bbox=dict(facecolor='#fcc36d', edgecolor='#ff6200', boxstyle='round, pad=0.5'))\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"silhouette_analysis(customer_data_pca, 3, 12, figsize=(20, 50))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #bbf2ef ; border-radius: 10px; padding: 15px; border: 1px solid #ccc; font-size: 16px;\">\n\n* To deal with multucollinear features, we will go ahead with Principal Component Analysis\n* This would help in forming better clusters\n\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color: #3296e3; padding: 8px 16px; border-radius: 20px; box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1); text-align: left;\">\n  <h1 style=\"color: white; font-size: 24px;\"> Clustering Using K-Means </h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color: #bbf2ef ; border-radius: 10px; padding: 15px; border: 1px solid #ccc; font-size: 16px;\">\n\n* To deal with multucollinear features, we will go ahead with Principal Component Analysis\n* This would help in forming better clusters\n\n</div>","metadata":{}},{"cell_type":"code","source":"from collections import Counter\n\n### Apply KMeans clustering using the optimal k\nkmeans = KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=100, random_state=0)\nkmeans.fit(customer_data_pca)\n\n### Get the frequency of each cluster\ncluster_frequencies = Counter(kmeans.labels_)\n\n### Create a mapping from old labels to new labels based on frequency\nlabel_mapping = {label: new_label for new_label, (label, _) in \n                 enumerate(cluster_frequencies.most_common())}\n\n### Reverse the mapping to assign labels as per your criteria\nlabel_mapping = {v: k for k, v in {2: 1, 1: 0, 0: 2}.items()}\n\n### Apply the mapping to get the new labels\nnew_labels = np.array([label_mapping[label] for label in kmeans.labels_])\n\n### Append the new cluster labels back to the original dataset\ncustomer_data_cleaned['cluster'] = new_labels\n\n### Append the new cluster labels to the PCA version of the dataset\ncustomer_data_pca['cluster'] = new_labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customer_data_cleaned.head(4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customer_data_cleaned['cluster'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customer_data_cleaned.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #0000ff; padding: 10px 20px; border-radius: 20px; box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1); text-align: center;\">\n  <h1 style=\"color: white; font-size: 30px;\"> Clustering Evaluation </h1>\n</div>","metadata":{}},{"cell_type":"code","source":"### Setting up the color scheme for the clusters (RGB order)\n\ncolors = ['#e8000b', '#1ac938', '#023eff']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.graph_objects as go\n\n### Create separate data frames for each cluster\ncluster_0 = customer_data_pca[customer_data_pca['cluster'] == 0]\ncluster_1 = customer_data_pca[customer_data_pca['cluster'] == 1]\ncluster_2 = customer_data_pca[customer_data_pca['cluster'] == 2]\n\n### Create a 3D scatter plot\nfig = go.Figure()\n\n### Add data points for each cluster separately and specify the color\nfig.add_trace(go.Scatter3d(x=cluster_0['PC1'], y=cluster_0['PC2'], z=cluster_0['PC3'], \n                           mode='markers', marker=dict(color=colors[0], size=5, opacity=0.4), name='Cluster 0'))\nfig.add_trace(go.Scatter3d(x=cluster_1['PC1'], y=cluster_1['PC2'], z=cluster_1['PC3'], \n                           mode='markers', marker=dict(color=colors[1], size=5, opacity=0.4), name='Cluster 1'))\nfig.add_trace(go.Scatter3d(x=cluster_2['PC1'], y=cluster_2['PC2'], z=cluster_2['PC3'], \n                           mode='markers', marker=dict(color=colors[2], size=5, opacity=0.4), name='Cluster 2'))\n\n### Set the title and layout details\nfig.update_layout(\n    title=dict(text='3D Visualization of Customer Clusters in PCA Space', x=0.5),\n    scene=dict(\n        xaxis=dict(backgroundcolor=\"#fcf0dc\", gridcolor='white', title='PC1'),\n        yaxis=dict(backgroundcolor=\"#fcf0dc\", gridcolor='white', title='PC2'),\n        zaxis=dict(backgroundcolor=\"#fcf0dc\", gridcolor='white', title='PC3'),\n    ),\n    width=900,\n    height=800\n)\n\n### Show the plot\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Calculate the percentage of customers in each cluster\ncluster_percentage = (customer_data_pca['cluster'].value_counts(normalize=True) * 100).reset_index()\ncluster_percentage.columns = ['Cluster', 'Percentage']\ncluster_percentage.sort_values(by='Cluster', inplace=True)\n\n### Create a horizontal bar plot\nplt.figure(figsize=(10, 4))\nsns.barplot(x='Percentage', y='Cluster', data=cluster_percentage, orient='h', palette=colors)\n\n### Adding percentages on the bars\nfor index, value in enumerate(cluster_percentage['Percentage']):\n    plt.text(value+0.5, index, f'{value:.2f}%')\n\nplt.title('Distribution of Customers Across Clusters', fontsize=14)\nplt.xticks(ticks=np.arange(0, 50, 5))\nplt.xlabel('Percentage (%)')\n\n### Show the plot\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #bbf2ef ; border-radius: 10px; padding: 15px; border: 1px solid #ccc; font-size: 16px;\">\n\n* To deal with multucollinear features, we will go ahead with Principal Component Analysis\n* This would help in forming better clusters\n\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color: #3296e3; padding: 8px 16px; border-radius: 20px; box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1); text-align: left;\">\n  <h1 style=\"color: white; font-size: 24px;\"> Evaluation Metrics </h1>\n</div>","metadata":{}},{"cell_type":"code","source":"from tabulate import tabulate\n\n### Compute number of customers\nnum_observations = len(customer_data_pca)\n\n### Separate the features and the cluster labels\nX = customer_data_pca.drop('cluster', axis=1)\nclusters = customer_data_pca['cluster']\n\n### Compute the metrics\nsil_score = silhouette_score(X, clusters)\ncalinski_score = calinski_harabasz_score(X, clusters)\ndavies_score = davies_bouldin_score(X, clusters)\n\n### Create a table to display the metrics and the number of observations\ntable_data = [\n    [\"Number of Observations\", num_observations],\n    [\"Silhouette Score\", sil_score],\n    [\"Calinski Harabasz Score\", calinski_score],\n    [\"Davies Bouldin Score\", davies_score]\n]\n\n### Print the table\nprint(tabulate(table_data, headers=[\"Metric\", \"Value\"], tablefmt='pretty'))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #bbf2ef ; border-radius: 10px; padding: 15px; border: 1px solid #ccc; font-size: 16px;\">\n\n* To deal with multucollinear features, we will go ahead with Principal Component Analysis\n* This would help in forming better clusters\n\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color: #0000ff; padding: 10px 20px; border-radius: 20px; box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1); text-align: center;\">\n  <h1 style=\"color: white; font-size: 30px;\"> Cluster Analysis </h1>\n</div>","metadata":{}},{"cell_type":"code","source":"### Setting 'CustomerID' column as index and assigning it to a new dataframe\ndf_customer = customer_data_cleaned.set_index('CustomerID')\n\n### Standardize the data (excluding the cluster column)\nscaler = StandardScaler()\ndf_customer_standardized = scaler.fit_transform(df_customer.drop(columns=['cluster'], axis=1))\n\n### Create a new dataframe with standardized values and add the cluster column back\ndf_customer_standardized = pd.DataFrame(df_customer_standardized, columns=df_customer.columns[:-1], index=df_customer.index)\ndf_customer_standardized['cluster'] = df_customer['cluster']\n\n### Calculate the centroids of each cluster\ncluster_centroids = df_customer_standardized.groupby('cluster').mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Function to create a radar chart\ndef create_radar_chart(ax, angles, data, color, cluster):\n    # Plot the data and fill the area\n    ax.fill(angles, data, color=color, alpha=0.4)\n    ax.plot(angles, data, color=color, linewidth=2, linestyle='solid')\n    \n    ### Add a title\n    ax.set_title(f'Cluster {cluster}', size=20, color=color, y=1.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Set data\nlabels=np.array(cluster_centroids.columns)\nnum_vars = len(labels)\n\n### Compute angle of each axis\nangles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n\n### The plot is circular, so we need to \"complete the loop\" and append the start to the end\nlabels = np.concatenate((labels, [labels[0]]))\nangles += angles[:1]\n\n### Initialize the figure\nfig, ax = plt.subplots(figsize=(20, 10), subplot_kw=dict(polar=True), nrows=1, ncols=3)\n\n### Create radar chart for each cluster\nfor i, color in enumerate(colors):\n    data = cluster_centroids.loc[i].tolist()\n    data += data[:1]  # Complete the loop\n    create_radar_chart(ax[i], angles, data, color, i)\n\n### Add input data\nax[0].set_xticks(angles[:-1])\nax[0].set_xticklabels(labels[:-1])\n\nax[1].set_xticks(angles[:-1])\nax[1].set_xticklabels(labels[:-1])\n\nax[2].set_xticks(angles[:-1])\nax[2].set_xticklabels(labels[:-1])\n\n### Add a grid\nax[0].grid(color='grey', linewidth=0.5)\n\n### Display the plot\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #bbf2ef ; border-radius: 10px; padding: 15px; border: 1px solid #ccc; font-size: 16px;\">\n\n* To deal with multucollinear features, we will go ahead with Principal Component Analysis\n* This would help in forming better clusters\n\n</div>","metadata":{}}]}